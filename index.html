<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="keywords" content="Adrian, de Wynter, Adrian de Wynter">
  <meta name="description" content="Adrian de Wynter's home page, updated every two years!">
  <meta name="google-site-verification" content="UhAhnWA093u8hR6zIezUozC_aTIl1dT1rPX8UEuOdDg" />
  <title style="font-family:helvetica">Adrian de Wynter</title>
  <meta name="viewport" content="width=device-width">
  <link rel="icon" type="image/png" href="dewynter.jpg"/>
  <link href="notes/stylesheet.css" rel="stylesheet" type="text/css"/>

  <style type="text/css">
    .profile_pic {
        float:right;
        width:200px;
        margin-left: 2.5%;
        margin-right: 2.5%;
        background-color: white;
        box-shadow: 0 4px 14px 0 rgba(0, 0, 0, 0.2), 0 6px 20px 0 rgba(0, 0, 0, 0.19);
    }
  </style>

</head>


<body>
    <div class="wrap">
    <div class="page-content">
    <h1 >Adrian de Wynter</h1>
<a href="dewynter.jpg" target="_blank"><img class="profile_pic" src="dewynter.jpg"/></a>
    </div>
    </div>

    <div class="text_div">
    <p style="font-family:helvetica">I am a principal applied scientist at Microsoft and a researcher (PGR) at the University of York. I work on projects addressing natural language understanding/generation and fundamental problems in deep learning, such as reasoning and formal modelling of LLMs.</p>

    <p style="font-family:helvetica">My primary research interest lies within computation, and specifically, the development of algorithms and meta-algorithms for machine learning. My approach is mainly intuitionistic in nature, contrasting with some other formalisms used in this field. Namely, algorithms should have provable guarantees of complexity and convergence via construction, and this proof must be closely-related to a <em>computable</em> (e.g., realistic, decidable, production) scenario. This has the advantage of providing feasible, meaningful statements about complex problems, while at the same time circumventing mathematical results that are rarely, or at all, seen in practice.</p>

    <p style="font-family:helvetica">I'm a strong proponent of training small and efficient models, as opposed to overspecified networks--which I call <a href="https://www.youtube.com/watch?v=_oNgyUAEv0Q">Jurassic</a> networks--via the development of algorithms with provable optimality guarantees. Here "efficient" would mean "only as big as needed for the task". This is important because the power required to train these huge models can be translated directly into tons of carbon emitted into the atmosphere, and it's <a href="https://arxiv.org/abs/1906.02243">devastating</a> to the environment.</p>

    <p style="font-family:helvetica"> Although I <a href="https://arxiv.org/pdf/1908.09942.pdf">showed</a> that finding a globally optimal solution to this problem is undecidable in its general form, I have also <a href="https://arxiv.org/pdf/2010.08512">proved</a> that for several interesting cases it is possible to find approximation algorithms that give near-optimal solutions in polynomial time--going as far as <a href="https://arxiv.org/pdf/2010.10499">applying</a> these results to the well-known BERT language model and reaching a new state-of-the-art on model compression. This last contribution was later adapted for quantum circuit optimization in a rather fantastic <a href="https://github.com/XanaduAI/QHack2021/issues/73">work</a> by folks at ORNL.</p>

    <p style="font-family:helvetica">Other research interests of mine are related to preserving endangered languages; as well as applications of LLMs to foster a more inclusive environment to traditionally excluded groups in ML research and application (e.g., neurodiverse individuals such as myself, non-English speakers, etcetera).</p>

    <p style="font-family:helvetica">Last updated: Dec '23.<br/>
  </div>

<p></p>
<hr>

<h2>Posts</h2>

<div class="text_div">
<p style="font-family:helvetica"> I've found it useful to have a series of "posts" on the work I do, to make it more accessible and share my passion for mathematics. Especially since I don't have any social media (does LinkedIn count?)... I'm absolutely terrible at updating this site (record: 2 years), so bear with me.</p>

<ul>
  <li><p class="liblog" style="font-family:helvetica"><a href="https://adewynter.github.io/notes/civ_utms.html">Turing Completeness and Sid Meier's Civilization</a>: A brief note about my paper "Turing Completeness and Sid Meier's Civilization". We talk about how to execute arbitrary algorithms inside Civ, and what does that mean for this and other 4X games.</p>
  </li>
  <li><p class="liblog" style="font-family:helvetica"><a href="https://adewynter.github.io/notes/decidability_of_nas.html">Some Computational Aspects of NAS</a>: A post on how hard neural architecture search (NAS) and machine learning can be, from a computational perspective. It also discusses the workarounds and applications of this result, with a particular emphasis on why some NAS approaches do not do better than random search. This is a summary of my poorly-titled, ever-misinterpreted paper "On The Bounds of Function Approximations."</p>
  </li>
  <li><p class="liblog" style="font-family:helvetica"><a href="https://adewynter.github.io/notes/bort_algorithms_and_applications.html">Bort: Algorithms and Applications</a>: a post on the algorithms used to obtain Bort, an optimally compressed version of the BERT language model. This can be viewed as a summary of my papers "Optimal Subarchitecture Extraction for BERT", "An Algorithm for Learning Smaller Representations of Models With Scarce Data", and "An Approximation Algorithm for Optimal Subarchitecture Extraction", albeit less concise than these titles, if you can believe it.</p>
  </li>
</ul>
</div>


<p></p>
<hr>

<h2 style="font-family:helvetica">Selected Publications and Talks</h2>

<div class="text_div">
<p style="font-family:helvetica">Following Larry Wasserman's <a href="http://stat.cmu.edu/~larry/Peer-Review.pdf">essay</a>, I invite comments on the papers below. Feel free to email me.<br>
For a longer list of publications see <a href="notes/older_papers.html">here</a>. For how to handle my last name's weird spelling rules, see <a href="notes/tussenvoegsels.html">here</a>.</p>
  <table style="font-family:helvetica">

    <tr>
      <td><b>2023&nbsp;&nbsp;</b></td>
      <td>
        <b>On Meta-Prompting
        <a href="https://arxiv.org/abs/2312.06562">[pdf]</a>
        <a href="bibtex/metaprompting.html">[BibTex]</a>
        <a href="https://github.com/adewynter/metaprompting">[Code]</a></b>
        <br/>Adrian de Wynter, Xun Wang, Qilong Gu, and Si-Qing Chen
        <br/>Preprint
        <br /><br />
      </td>
    </tr>

    <tr>
      <td><b></b></td>
      <td>
        <b>A User-Centered Evaluation of Spanish Text Simplification 
        <a href="https://arxiv.org/abs/2308.07556">[pdf]</a>
        <a href="bibtex/breve.html">[BibTex]</a>
        <a href="https://github.com/microsoft/BrevE-CLaro">[Data]</a></b>
        <br />Adrian de Wynter, Anthony Hevia, and Si-Qing Chen
        <br />Preprint
        <br /><br />
      </td>
    </tr>

    <tr>
      <td><b></b></td>
      <td>
        <b>An Evaluation of LLM Outputs: Discourse and Memorization 
          <a href="https://www.sciencedirect.com/science/article/pii/S2949719123000213">[pdf]</a>
          <a href="bibtex/llmeval.html">[BibTex]</a></b>
          <br />Adrian de Wynter, Xun Wang, Alex Sokolov, Qilong Gu, and Si-Qing Chen
          <br />The Natural Language Processing Journal
          <br /><br />
      </td>
    </tr>

    <tr>
      <td><b></b></td>
      <td>
        <b>On the Opportunities and Dangers of LLM-Based Evaluation</b>
        <br/>Chris Quirk and Adrian de Wynter
        <br/>Invited talk at the 2023 MLADS Conference
        <br /><br />
      </td>
    </tr>


    <tr>
      <td><b></b></td>
      <td>
        <b>Are Large Language Model-based Evaluators the Solution to Scaling Up Multilingual Evaluation?
        <a href="https://arxiv.org/abs/2309.07462">[pdf]</a>
        <a href="bibtex/llmmultilingual.html">[BibTex]</a></b>
        <br />Rishav Hada, Varun Gumma, Adrian de Wynter, Harshita Diddee, Mohamed Ahmed, Monojit Choudhury, Kalika Bali, and Sunayana Sitaram
        <br />Preprint
        <br /><br />
      </td>
    </tr>

    <tr>
      <td><b></b></td>
      <td>
        <b>"I Wish To Have An Argument!": Argumentative Reasoning in Large Language Models
        <a href="https/:/arxiv.org/abs/2309.16938">[pdf]</a>
        <a href="bibtex/argumentation1.html">[BibTex]</a>
        <a href="https://github.com/adewynter/argumentation-llms">[Code]</a></b>
        <br />Adrian de Wynter and Tommy Yuan
        <br />Preprint
        <br /><br />
      </td>
    </tr>

    <tr>
      <td><b></b></td>
      <td>
        <b>The Curse of the Biased Researcher</b>
        <br/>Adrian de Wynter
        <br/>Invited talk at the 2023 MLADS Conference<br />
        <br /><br />
      </td>
    </tr>

    <tr>
      <td><b>2022&nbsp;&nbsp;</b></td>
      <td>
        <b>Turing Completeness and <i>Sid Meier's Civilization</i>
        <a href="https://ieeexplore.ieee.org/document/9756289">[pdf]</a>
        <a href="bibtex/civ.html">[BibTex]</a>
        <a href="https://adewynter.github.io/notes/img/BB3.gif">[The Turing Machine in Action]</a></b>
        <br />Adrian de Wynter
        <br />IEEE Transactions on Games
        <br /><br />
      </td>
    </tr>

    <tr>
      <td><b>2020&nbsp;&nbsp;</b></td>
      <td>
        <b>Optimal Subarchitecture Extraction for BERT
        <a href="https://arxiv.org/pdf/2010.10499">[pdf]</a>
        <a href="bibtex/osebert.html">[BibTex]</a>
        <a href="https://github.com/alexa/bort">[Code]</a></b>
        <br />Adrian de Wynter and Daniel J. Perry
        <br />Preprint
        <br /><br />
      </td>
    </tr>

    <tr>
      <td><b></b></td>
      <td>
        <b>An Algorithm for Learning Smaller Representations of Models With Scarce Data 
        <a href="https://arxiv.org/pdf/2010.07990">[pdf]</a>
        <a href="bibtex/agora.html">[BibTex]</a></b>
        <br />Adrian de Wynter
        <br />Preprint
        <br /><br />
      </td>
    </tr>

    <tr>
      <td><b>2019&nbsp;&nbsp;</b></td>
      <td>
        <b>On the Bounds of Function Approximations 
        <a href="https://arxiv.org/pdf/1908.09942.pdf">[pdf]</a>
        <a href="bibtex/fa.html">[BibTex]</a></b>
        <br />Adrian de Wynter<br />ICANN 2019 (oral presentation)
        <br /><br />
      </td>
    </tr>

</table>
</div>

<p></p>
<hr>

<h2 style="font-family:helvetica">Selected Press Coverage</h2>

<div class="text_div" style="font-family:helvetica">
<p>Some coverage of the work I do, in case my posts remain as confusing as the original papers.</p>
<ul>
  <li><p class="liblog" style="font-family:helvetica">
    <b><a href="https://www.amazon.science/blog/a-version-of-the-bert-language-model-thats-20-times-as-fast">A version of the BERT language model that’s 20 times as fast</a></b> - Another post edited by Larry Hardesty. This one talks about Bort.</p>
  </li>

  <li><p class="liblog" style="font-family:helvetica">
    <b><a href="https://www.infoq.com/articles/state-art-automl/">State of the Art in Automated Machine Learning</a></b> - This is an interview I, along with other researchers, gave for InfoQ around AutoML. It's so interesting to see people of such different backgrounds arriving to the same conclusions :)</p>
  </li>

  <li><p class="liblog" style="font-family:helvetica">
    <b><a href="https://www.infoq.com/news/2019/11/alexa-genetic-deep-learning/">Alexa Research Paper Shows Genetic Algorithms Offer Best Solution for Neural Network Optimization</a></b> - This post sums up very nicely my work around NAS/ASP/FA.</p>
  </li>

  <li><p class="liblog" style="font-family:helvetica">
    <b><a href="https://venturebeat.com/2019/09/23/amazon-researchers-say-evolutionary-approach-improves-the-selection-of-ai-models/">Amazon researchers say evolutionary approach improves the selection of AI models</a></b> - From Venturebeat.
  </p>
  </li>

  <li><p class="liblog" style="font-family:helvetica">
      <b><a href="https://www.amazon.science/blog/how-to-construct-the-optimal-neural-architecture-for-your-machine-learning-task">How to Construct the Optimal Neural Architecture for Your Machine Learning Task</a></b> - A post edited by the awesome Larry Hardesty.</p>
  </li>
</ul>
</div>

<p></p>
<hr>


<div class="raligned">
<p style="font-family:helvetica">Contact: first-initial-full-last-name-including-tussenvoegsel (at) microsoft.com</p>
</div>

<div  style="font-family:helvetica"class="wrap">
Factoid: my ORCID (326797241) is a prime number; it is expressible as the sum of two squares (1715 and 17996); and it is the square root (hypothenuse) of the sum of two squares (61726280 and 320914791). Yay.<br/>
</p>
</div>

<font size=0 color=#ffffff>Adrian, de Wynter, Adrian de Wynter, category theory, LLMs, NAS, meta-learning, Bort, Agora, intuitionistic machine learning</font>

</body>

</html>
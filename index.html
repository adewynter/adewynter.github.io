<!DOCTYPE html>
<html>
  <head>
  <title>Adrian de Wynter</title>
  
  <meta name="google-site-verification" content="UhAhnWA093u8hR6zIezUozC_aTIl1dT1rPX8UEuOdDg" />
  <meta content="Adrian de Wynter" name="author">
  <meta name="keywords" content="Adrian, de Wynter, Adrian de Wynter">
  <meta name="description" content="Adrian de Wynter's home page, updated every two years!">
  <meta content="width=device-width, initial-scale=1.0, user-scalable=no" name="viewport">
  
  <link href="styles/main_index.css" media="screen" rel="stylesheet" type="text/css">
  <link rel="icon" type="image/png" href="img/dewynter.jpg"/>

  </head>

<!-- very loosely taken from Ana Enders' design -- anaenders.com -->
  <body>
    <div class="content">

      <div class="header-wrapper">
        <header>
          <div style="display: flex; justify-content: center; text-align: center; align-items: center;"><h1>Adrian de Wynter</h1>
             
            <p> <br/><br/>.</p>
          </div>
        </header>
      </div>

      <div>
        <div class="banner">
          <ul>
            <li class="slide2"></li>
            <li class="slide1"></li>
          </ul>
        </div>
        <div class="banner2">
          <ul>
          <li class="mslide1"></li>
          <li class="mslide2"></li>
        </ul>
        </div>

        <div class="main-text">
          <div class="about">
            <div class="about-text">

              <p style="font-family:helvetica">I am a principal applied <b>scientist</b> at Microsoft and a <b>researcher</b> (PGR) at the University of York. I work in projects related to natural language understanding/generation and fundamental problems in AI, such as <b>reasoning and formal modelling of dialogue</b>, like LLMs.</p>

              <p style="font-family:helvetica">At Microsoft my work involves leading, designing, and deploying Word- and Office- AI features and research. These deal with composition (what you see when you type in Word), multilinguality (e.g., expanding products to new markets), measurement (reasoning, automated evaluation, meta-evaluation and trust), personalisation (measurement, implementation, characterisation), and other workstreams. Yes, I also work on buzzwords like 'agentic workflows'. Most of the work here, you can see it in Word Copilot.</p>

              <p style="font-family:helvetica">My primary research interest is <b>reasoning as it relates to language in humans and machines</b>. Lately I have focused on LLM-based reasoning capabilities (e.g. <a href="https://ojs.aaai.org/index.php/AAAI/article/view/35011">here</a>, <a href="https://arxiv.org/abs/2507.01936">here</a>, and <a href="https://arxiv.org/abs/2404.01230">here</a>). <a href="https://arxiv.org/abs/2509.10414">This one</a> is my favourite so far--a formal evaluation of in-context learning, with interesting results (e.g., it <em>does</em> work, but not like most papers have claimed!)
              <br>
              My theoretical work is intuitionistic: algorithms have guarantees of complexity and convergence via <em>constructive proofs</em>, and must relate to a <em>realistic</em> (e.g. production) scenario. This gives <b>meaningful answers about complex problems</b>.</p>
            </div>

            <div class="about-text">
              <p style="font-family:helvetica">For example, we used <a href="https://arxiv.org/abs/2312.06562">category theory</a> to prove that <b>some prompting strategies are objectively better than others</b>; and that they would produce more preferrable outcomes by users (and ended up being a product in Word). I also recently wrote <a href="https://arxiv.org/abs/2506.03083">an algorithm</a> with cryptographic guarantees for determining <b>trust in LLMs-as-judges</b>.</p>

              <p style="font-family:helvetica">In earlier work I <a href="https://arxiv.org/abs/1908.09942">showed</a> that finding a globally optimal solution to model compression is undecidable, but <a href="https://arxiv.org/abs/2010.08512">proved</a> that polynomial-time approximation algorithms exist--and <a href="https://arxiv.org/abs/2010.10499">applied</a> these results to BERT and reaching a (then) state-of-the-art on model compression. This last contribution was later adapted for quantum circuit optimisation in <a href="https://github.com/XanaduAI/QHack2021/issues/73">work</a> at ORNL.
                I also showed (bridging learning theory and topological data analysis) how (and when) <a href="https://link.springer.com/article/10.1007/s41884-024-00153-0">LLM-based data augmentation</a> works.</p>

              <p style="font-family:helvetica">My other research interests relate to <b>recreational mathematics</b> (games), <b>preserving endangered languages</b>, and <b>computational social science</b>. In the latter I have worked on mitigating <a href="https://ojs.aaai.org/index.php/AAAI/article/view/35011">toxicity</a>, <a href="https://arxiv.org/abs/2410.11005">unfairness</a>, and <a href="https://arxiv.org/abs/2503.20302">other</a> harms of LLMs; <a href="https://arxiv.org/abs/2508.06374">personalisation</a> and <a href="https://arxiv.org/abs/2507.01936">sycophancy</a> in LLMs; <a href="https://arxiv.org/abs/2408.15409">research on LLM research</a>; and worked on the first study of the impact of ChatGPT on <a href="https://arxiv.org/abs/2412.01617">loneliness</a>. And I also publish in <a href="https://sigbovik.org/2025/proceedings.pdf">SIGBOVIK</a>, because this job is actually fun.</p>

              <p style="font-family:helvetica">In terms of service, I have (obviously) served as reviewer for AAAI, *ACL, ICLR and so on. I also review for Nature (Communications, Artificial Intelligence), IEEE Transactions on Games, and ACM TIST.</p>

              <p style="font-family:helvetica">And if you are stalking me, here's my <a href="https://scholar.google.com/citations?hl=nl&user=Vg_EOYsAAAAJ">Google Scholar</a>. Coverage by the media on my work is <a href="#media-coverage">below</a>. My X is somewhere floating on the internet, and I also run <a href="https://adriandewynter.substack.com">&lt;TODO: insert title&gt;</a>, a newsletter on AI papers and news. If you are stalking me, this is as close as you'll get to my socials (I have none). Just note that while the anecdotes are fake, the opinions are not. Don't forget to subscibe!</p>

              <p style="font-family:Helvetica, Serif; color: #231f20; display:flex; justify-content:flex-end">Last updated: Jan '26.</p>
            </div>
          </div>




          <div class="papers">

              <div class="papers-block">
                <div class="section-label--right">New and Selected Works</div>
                <div class="papers-education--desktop">

                  <div><p></p><p style="font-family: Helvetica"><i>Following Larry Wasserman's <a href="http://stat.cmu.edu/~larry/Peer-Review.pdf">essay</a>, I invite comments on the papers below. Feel free to email me.<br>For a longer, <b>complete list of works</b> see <a href="notes/older_papers.html">here</a>.<br>For how to handle my last name's weird spelling rules, see <a href="notes/tussenvoegsels.html">here</a>.</i></p></div>


                  <div class="section-item">
                    <div class="paper-title">
                      <b>Causal Reasoning Favors Encoders: On The Limits of Decoder-Only Models</b>
                    </div>
                    <div class="paper-misc">
                      <a href="https://arxiv.org/abs/2512.10561">[pdf]</a>
                      <a href="bibtex/caussl.html">[BibTex]</a></b>
                      <!-- <a href="https://github.com/adewynter/is-icl-learning">[Code]</a></b> -->
                    </div>
                    <div class="paper-authors">
                      Amartya Roy, Elamparithy M, Kripabandhu Ghosh, Ponnurangam Kumaraguru, and <b>Adrian de Wynter</b>
                    </div>
                    <div class="paper-misc">
                      Preprint (2025)
                    </div>
                  </div>


                  <div class="section-item">
                    <div class="paper-title">
                      <b>On Interpreting Measurement with LLMs</b>
                    </div>
                    <div class="paper-misc">
                      <a href="assets/Measurement%20in%20LLMs.pdf">[slides]</a>
                    </div>
                    <div class="paper-authors">
                      Adrian de Wynter
                    </div>
                    <div class="paper-misc">
                      Invited talk at MBZUAI's research showcase (2025)
                    </div>
                  </div>


                  <div class="section-item">
                    <div class="paper-title">
                      <b>Is In-Context Learning Learning?</b>
                    </div>
                    <div class="paper-misc">
                      <a href="https://arxiv.org/abs/2509.10414">[pdf]</a>
                      <a href="bibtex/icl.html">[BibTex]</a></b>
                      <a href="https://github.com/adewynter/is-icl-learning">[Code]</a></b>
                      <a href="#media-coverage-icl">[Press coverage]</a>
                    </div>
                    <div class="paper-authors">
                      Adrian de Wynter
                    </div>
                    <div class="paper-misc">
                      ICLR (2026). Chosen by the Turing Post as one of 2025's <a href="#media-coverage-icl">AI papers you must read</a>.
                    </div>
                  </div>

                  <div class="section-item">
                    <div class="paper-title">
                      <b>Evaluating Style-Personalized Text Generation: Challenges and Directions</b>
                    </div>
                    <div class="paper-misc">
                      <a href="https://arxiv.org/abs/2508.06374">[pdf]</a>
                      <!-- <a href="bibtex/thinline.html">[BibTex]</a></b> -->
                      </div>
                    <div class="paper-authors">
                      Anubhav Jangra, Bahareh Sarrafzadeh, Silviu Cucerzan, <b>Adrian de Wynter</b>*, and Sujay Kumar Jauhar*
                    </div>
                    <div class="paper-misc">
                      Preprint (2025)
                    </div>
                  </div>

                  <div class="section-item">
                    <div class="paper-title">
                      <b>The Thin Line Between Comprehension and Persuasion in LLMs</b>
                    </div>
                    <div class="paper-misc">
                      <a href="https://arxiv.org/abs/2507.01936">[pdf]</a>
                      <a href="bibtex/thinline.html">[BibTex]</a></b>
                      <a href="https://github.com/adewynter/the_thin_line">[Code]</a></b>
                      <a href="#media-coverage-persuasion">[Press coverage]</a>
                    </div>
                    <div class="paper-authors">
                      Adrian de Wynter and Tangming Yuan
                    </div>
                    <div class="paper-misc">
                      Preprint (2025)
                    </div>
                  </div>

                  <div class="section-item">
                    <div class="paper-title">
                      <b>Labelling Data with Unknown References</b>
                    </div>
                    <div class="paper-misc">
                      <a href="https://arxiv.org/abs/2506.03083">[pdf]</a>
                      <a href="bibtex/unknown.html">[BibTex]</a></b>
                       <a href="https://github.com/adewynter/no_data_algorithm">[Code]</a>
                      <a href="#media-coverage-no-data">[Press coverage]</a>
                     </b> 
                    </div>
                    <div class="paper-authors">
                      Adrian de Wynter
                    </div>
                    <div class="paper-misc">
                      Preprint (2025)
                    </div>
                  </div>

                  <div class="section-item">
                    <div class="paper-title">
                      <b>Does using LLMs in daily life help or hinder learning a second language?</b>
                    </div>
                    <div class="paper-misc">
                      <a href="https://escholarship.org/uc/item/2jh361fg">[pdf]</a>
                      <a href="https://escholarship.org/uc/item/2jh361fg">[BibTex]</a></b>
                      <!-- <a href="https://github.com/adewynter/awes_laws_and_flaws">[Code]</a></b> -->
                    </div>
                    <div class="paper-authors">
                      Wei Li, Andy Zhao, <b>Adrian de Wynter</b>, Si-Qing Chen, Paul Karimov, Paul and Joshua K. Hartshorne 
                    </div>
                    <div class="paper-misc">
                      Proceedings of the Annual Meeting of the Cognitive Science Society (2025)
                    </div>
                  </div>


                  <div class="section-item">
                    <div class="paper-title">
                      <b>A Multilingual, Culture-First Approach to Addressing Misgendering in LLM Applications</b>
                    </div>
                    <div class="paper-misc">
                      <a href="https://arxiv.org/abs/2503.20302">[pdf]</a>
                      <a href="bibtex/misgendering.html">[BibTex]</a></b>
                      <a href="https://github.com/microsoft/Multilingual-Culture-First-Misgendering-Guardrails">[Code]</a></b>
                    </div>
                    <div class="paper-authors">
                      Sunayana Sitaram, <b>Adrian de Wynter</b>, Isobel McCrum, Qilong Gu, and Si-Qing Chen
                    </div>
                    <div class="paper-misc">
                      EMNLP 2025 Main
                    </div>
                  </div>

                  <div class="section-item">
                    <div class="paper-title">
                      <b>If Eleanor Rigby Had Met ChatGPT: A Study on Loneliness in a Post-LLM World</b>
                    </div>
                    <div class="paper-misc">
                      <a href="https://arxiv.org/abs/2412.01617">[pdf]</a>
                      <a href="https://aclanthology.org/2025.acl-long.976/">[BibTex]</a></b>
                      <a href="https://github.com/adewynter/EleanorRigby">[Code]</a></b>
                    </div>
                    <div class="paper-authors">
                      Adrian de Wynter
                    </div>
                    <div class="paper-misc">
                      ACL 2025 Main
                    </div>
                  </div>

                  <div class="section-item">
                    <div class="paper-title">
                      <b>Awes, Laws and Flaws of Today's LLM Research</b>
                    </div>
                    <div class="paper-misc">
                      <a href="https://arxiv.org/abs/2408.15409">[pdf]</a>
                      <a href="https://aclanthology.org/2025.findings-acl.664/">[BibTex]</a></b>
                      <a href="https://github.com/adewynter/awes_laws_and_flaws">[Code]</a></b>
                    </div>
                    <div class="paper-authors">
                      Adrian de Wynter
                    </div>
                    <div class="paper-misc">
                      ACL 2025 Findings
                    </div>
                  </div>

                  <div class="section-item">
                    <div class="paper-title">
                      <b>LLMs Are All You Need</b>
                    </div>
                    <div class="paper-misc">
                      <a href="https://sigbovik.org/2025/proceedings.pdf">[pdf]</a>
                    </div>
                    <div class="paper-authors">
                      Adrian de Wynter
                    </div>
                    <div class="paper-misc">
                      SIGBOVIK 2025 (IKYK)
                    </div>
                  </div>

                  <div class="section-item">
                    <div class="paper-title">
                      <b>Will GPT-4 Run DOOM?</b>
                    </div>
                    <div class="paper-misc">
                      <a href="https://ieeexplore.ieee.org/document/10752360">[pdf]</a>
                      <a href="bibtex/doom.html">[BibTex]</a>
                      <a href="https://github.com/adewynter/Doom">[Code]</a>
                      <a href="Doom.html">[Post]</a>
                      <a href="#media-coverage-doom">[Press coverage]</a>
                    </div>
                    <div class="paper-authors">
                      Adrian de Wynter
                    </div>
                    <div class="paper-misc">
                      IEEE Transactions on Games (2024).
                    </div>
                  </div>

                  <div class="section-item">
                    <div class="paper-title">
                      <b>LLM as a Mastermind: A Survey of Strategic Reasoning with Large Language Models</b>
                    </div>
                    <div class="paper-misc">
                      <a href="https://arxiv.org/pdf/2404.01230.pdf">[pdf]</a>
                      <a href="bibtex/llmmastermind.html">[BibTex]</a></b>
                    </div>
                    <div class="paper-authors">
                      Yadong Zhang, Shaoguang Mao, Tao Ge, Xun Wang, <b>Adrian de Wynter</b>, Yan Xia, Wenshan Wu, Ting Song, Man Lan and Furu Wei
                    </div>
                    <div class="paper-misc">
                      COLM 2024
                    </div>
                  </div>

                  <div class="section-item">
                    <div class="paper-title">
                      <b>Assessing Dialect Fairness and Robustness of Large Language Models in Reasoning Tasks</b>
                    </div>
                    <div class="paper-misc">
                      <a href="https://arxiv.org/abs/2410.11005">[pdf]</a>
                      <a href="https://aclanthology.org/2025.acl-long.317/">[BibTex]</a>
                    </div>
                    <div class="paper-authors">
                      Fangru Lin, Shaoguang Mao, Emanuele La Malfa, Valentin Hofmann, <b>Adrian de Wynter</b>, Xun Wang, Si-Qing Chen, Michael J. Wooldridge, Janet B. Pierrehumbert, Furu Wei
                    </div>
                    <div class="paper-misc">
                      ACL 2025 Main
                    </div>
                  </div>
                  
                  <div class="section-item">
                    <div class="paper-title">
                      <b>RTP-LX: Can LLMs Evaluate Toxicity in Multilingual Scenarios?</b>
                    </div>
                    <div class="paper-misc">
                      <a href="https://ojs.aaai.org/index.php/AAAI/article/view/35011">[pdf]</a>
                      <a href="https://ojs.aaai.org/index.php/AAAI/article/view/35011">[BibTex]</a>
                      <a href="https://huggingface.co/datasets/adewynter/RTP-LX">[Code]</a>
                    </div>
                    <div class="paper-authors">
                      Adrian de Wynter <i>et al.</i>
                    </div>
                    <div class="paper-misc">
                      AAAI 2025
                    </div>
                  </div>

                  <div class="section-item">
                    <div class="paper-title">
                      <b>On Meta-Prompting</b>
                    </div>
                    <div class="paper-misc">
                      <a href="https://arxiv.org/abs/2312.06562">[pdf]</a>
                      <a href="bibtex/metaprompting.html">[BibTex]</a>
                      <a href="https://github.com/adewynter/metaprompting">[Code]</a>
                    </div>
                    <div class="paper-authors">
                      Adrian de Wynter, Xun Wang, Qilong Gu, and Si-Qing Chen
                    </div>
                    <div class="paper-misc">
                      Preprint
                    </div>
                  </div>

                  <div class="section-item">
                    <div class="paper-title">
                      <b>An Evaluation of LLM Outputs: Discourse and Memorization</b>
                    </div>
                    <div class="paper-misc">
                      <a href="https://www.sciencedirect.com/science/article/pii/S2949719123000213">[pdf]</a>
                      <a href="bibtex/llmeval.html">[BibTex]</a>
                    </div>
                    <div class="paper-authors">
                      Adrian de Wynter, Xun Wang, Alex Sokolov, Qilong Gu, and Si-Qing Chen
                    </div>
                    <div class="paper-misc">
                      The Natural Language Processing Journal
                    </div>
                  </div>

                  <div class="section-item">
                    <div class="paper-title">
                      <b>"I'd Like to Have an Argument, Please": Argumentative Reasoning in Large Language Models</b>
                    </div>
                    <div class="paper-misc">
                      <a href="https://ebooks.iospress.nl/doi/10.3233/FAIA240311">[pdf]</a>
                      <a href="bibtex/argumentation1.html">[BibTex]</a>
                      <a href="https://github.com/adewynter/argumentation-llms">[Code]</a>
                    </div>
                    <div class="paper-authors">
                      Adrian de Wynter and Tangming Yuan
                    </div>
                    <div class="paper-misc">
                      COMMA 2024
                    </div>
                  </div>

                  <div class="section-item">
                    <div class="paper-title">
                      <b>Are Large Language Model-based Evaluators the Solution to Scaling Up Multilingual Evaluation?</b>
                    </div>
                    <div class="paper-misc">
                      <a href="https://arxiv.org/abs/2309.07462">[pdf]</a>
                      <a href="https://aclanthology.org/2024.findings-eacl.71/">[BibTex]</a>
                    </div>
                    <div class="paper-authors">
                      Rishav Hada, Varun Gumma, <b>Adrian de Wynter</b>, Harshita Diddee, Mohamed Ahmed, Monojit Choudhury, Kalika Bali, and Sunayana Sitaram
                    </div>
                    <div class="paper-misc">
                      EACL 2024
                    </div>
                  </div>

                  <div class="section-item">
                    <div class="paper-title">
                      <b>Turing Completeness and <i>Sid Meier's Civilization</i></b>
                    </div>
                    <div class="paper-misc">
                      <a href="https://ieeexplore.ieee.org/document/9756289">[pdf]</a>
                      <a href="bibtex/civ.html">[BibTex]</a>
                      <a href="https://adewynter.github.io/notes/img/BB3.gif">[The Turing Machine in Action]</a>
                    </div>
                    <div class="paper-authors">
                      Adrian de Wynter
                    </div>
                    <div class="paper-misc">
                      IEEE Transactions on Games
                    </div>
                  </div>

                  <div class="section-item">
                    <div class="paper-title">
                      <b>An Algorithm for Learning Smaller Representations of Models With Scarce Data </b>
                    </div>
                    <div class="paper-misc">
                      <a href="https://link.springer.com/article/10.1007/s41884-024-00153-0">[pdf]</a>
                      <a href="bibtex/agora.html">[BibTex]</a>
                      <a href="https://github.com/alexa/bort">[Code]</a>
                    </div>
                    <div class="paper-authors">
                      Adrian de Wynter
                    </div>
                    <div class="paper-misc">
                      Information Geometry (2024)
                    </div>
                  </div>

              </div><!-- papers-education--desktop -->

              <div><p style="font-family:helvetica; display:flex; justify-content:flex-end"><a href="notes/older_papers.html">More</a></p></div>

            </div><!-- papers-filler -->


          </div><!-- papers -->


          <div class="papers">
              <div class="papers-block">
                <div class="section-label--right">TL;DRs of Some Papers</div>

                <div class="papers-education--desktop">

                  <div>
                    <p></p>
                    <p style="font-family:helvetica"><i>I've found it useful to have a series of posts about some of my works. This makes them more accessible and allows me to share my passion for mathematics. I definitely do not proofread these.
                    <br>I'm absolutely terrible at updating this site (record: 2 years), so bear with me.</i></p>
                  </div>

                  <div class="section-item">
                    <div class="paper-title">
                      <b><a href="https://adewynter.github.io/notes/nodata.html">The No-Data Algorithm</a></b>
                    </div>
                    <div class="paper-misc">
                      How to enable trust in LLMs-as-judges WITHOUT labelled data! With proofs!
                    </div>
                  </div>

                  <div class="section-item">
                    <div class="paper-title">
                      <b><a href="https://adewynter.github.io/Doom.html">Will GPT-4 Run DOOM?</a></b>
                    </div>
                    <div class="paper-misc">
                      Yes but no. 
                      Links to code, resources, TL;DR of the paper, and videos of the model playing the game.
                    </div>
                  </div>

                  <div class="section-item">
                    <div class="paper-title">
                      <b><a href="https://adewynter.github.io/notes/civ_utms.html">Turing Completeness and Sid Meier's Civilization</a></b>
                    </div>
                    <div class="paper-misc">
                    Building a literal computer inside Civ
                    </div>
                  </div>


                  <div class="section-item">
                    <div class="paper-title">
                      <b><a href="https://adewynter.github.io/notes/decidability_of_nas.html">Neural architecture search is undecidable</a></b>
                    </div>
                    <div class="paper-misc">
                   <!-- A post on how hard neural architecture search (NAS) and machine learning can be, from a computational perspective. It also discusses the workarounds and applications of this result, with a particular emphasis on why some NAS approaches do not do better than random search. -->
                   A summary of my poorly-titled, ever-misinterpreted paper 'On The Bounds of Function Approximations.' 
                    </div>
                  </div>


                  <div class="section-item">
                    <div class="paper-title">
                      <b><a href="https://adewynter.github.io/notes/bort_algorithms_and_applications.html">Bort</a></b>
                    </div>
                    <div class="paper-misc">
                      (Provably) optimal model compression with algebraic topology
                      <!--
                      A post on the algorithms used to obtain Bort, an optimally compressed version of the BERT language model. This can be viewed as a summary of my papers "Optimal Subarchitecture Extraction for BERT", "An Algorithm for Learning Smaller Representations of Models With Scarce Data", and "An Approximation Algorithm for Optimal Subarchitecture Extraction", albeit less concise than these titles, if you can believe it. -->
                    </div>
                  </div>


              </div><!-- media-education--desktop -->

            </div><!-- media-filler -->


          </div><!-- media -->



          <div class="papers">
              <div class="papers-block">
                <div class="section-label--right" id="media-coverage">Selected Media Coverage</div>

                <div class="papers-education--desktop">

                  <div>
                    <p></p>
                    <p style="font-family: Helvetica"><i>Some media coverage of the work I do, in case my posts remain as confusing as the original papers.</i></p>
                  </div>

                  <div class="section-item" id="media-coverage-icl">
                    <div class="paper-title">
                      <b>The Turing Post selected this paper as one of the 23 papers from 2025 that indicate where is <a href="https://www.turingpost.com/p/fod133">AI headed</a>.</b>
                    </div>
                    <div class="paper-misc">
                    Posts covering my paper 'Is In-Context Learning Learning?'. Other coverage is <a href="https://levelup.gitconnected.com/do-llm-models-really-learn-microsoft-study-exposes-the-illusion-of-in-context-learning-in-llms-02bcb54e0a86">here</a> and <a href="https://min.news/en/tech/4b434e93cef12783b4b73455ba4f2f65.html">here</a>. I like this one, too: <a href="https://bdtechtalks.substack.com/p/llm-in-context-learning-icl-is-learning">LLM in-context learning (ICL) is learning, but not how you think</a>
                    </div>
                  </div>


                  <div class="section-item">
                    <div class="paper-title" id="media-coverage-no-data">
                      <b><a href="https://www.techinasia.com/news/microsofts-no-data-algorithm-enables-trust-without-labels">Microsoft’s No-Data Algorithm enables trust without labels</a></b>
                    </div>
                    <div class="paper-misc">
                    Brief coverage (with really cool implications I hadn't thought about) on the No-Data Algorithm. 
                    </div>
                  </div>

                  <div class="section-item">
                    <div class="paper-title" id="media-coverage-persuasion">
                      <b><a href="https://aibr.jp/2025/09/01/llmsにおける理解と説得の微妙な境界（the-thin-line-between-comprehension-and-persuasion-2/">LLMsにおける理解と説得の微妙な境界（The Thin Line Between Comprehension and Persuasion in LLMs）</a></b>
                    </div>
                    <div class="paper-misc">
                    A post talking about our paper 'The Thin Line Between Comprehension and Persuasion in LLMs' (in Japanese, but it isn't like Google Translate doesn't exist). Other coverage is <a href="https://quantumzeitgeist.com/large-language-models-assess-debate-revealing-dialogue-comprehension-limits/">here</a> (Quantum Zeitgeist). 
                    </div>
                  </div>


                  <div class="section-item">
                    <div class="paper-title" id="media-coverage-doom">
                      <b><a href="https://www.tweaktown.com/news/96799/microsoft-scientist-gets-ai-to-play-doom-but-then-issued-warning/index.html">Microsoft scientist gets AI to play DOOM but then issued a warning</a></b>
                    </div>

                    <div class="paper-misc">
                      Some of the coverage of the work I did with DOOM and GPT-4. You can also read about it <a href="https://www.tomshardware.com/tech-industry/artificial-intelligence/gpt-4v-can-play-doom-badly-doesnt-hesitate-to-shoot-humans-and-demons">here (Tom's Hardware)</a>, <a href="https://www.pcmag.com/news/openais-gpt-4-can-play-doom-but-only-like-a-noob">here (PC Mag)</a>, and <a href="https://www.theregister.com/2024/03/11/gpt4_wont_run_doom/">here (The Register)</a>.
                    </div>
                  </div>

                  <div class="section-item">
                    <div class="paper-title">
                      <b><a href="https://www.amazon.science/blog/a-version-of-the-bert-language-model-thats-20-times-as-fast">A version of the BERT language model that’s 20 times as fast</a></b>
                    </div>
                    <div class="paper-misc">
                      Another post edited by Larry Hardesty. This one talks about Bort.
                    </div>
                  </div>

                  <div class="section-item">
                    <div class="paper-title">
                      <b><a href="https://www.infoq.com/articles/state-art-automl/">State of the Art in Automated Machine Learning</a></b>
                    </div>
                    <div class="paper-misc">
                      This is an interview I, along with other researchers, gave for InfoQ around AutoML. It's so interesting to see people of such different backgrounds arriving to the same conclusions :)
                    </div>
                  </div>


                  <div class="section-item">
                    <div class="paper-title">
                      <b><a href="https://www.infoq.com/news/2019/11/alexa-genetic-deep-learning/">Alexa Research Paper Shows Genetic Algorithms Offer Best Solution for Neural Network Optimization</a></b>
                    </div>
                    <div class="paper-misc">
                    This post sums up very nicely my work around NAS/ASP/FA. Other coverage is <a href="https://venturebeat.com/2019/09/23/amazon-researchers-say-evolutionary-approach-improves-the-selection-of-ai-models/">here</a> (Venturebeat) and <a href="https://www.amazon.science/blog/how-to-construct-the-optimal-neural-architecture-for-your-machine-learning-task">here</a> (Amazon Science). 
                    </div>
                  </div>


              </div><!-- media-education--desktop -->

            </div><!-- media-filler -->


          </div><!-- media -->





          <div class="papers">

              <div class="papers-block">
              <p style="font-family:helvetica; display:flex; justify-content:flex-end">Contact: first-initial-full-last-name-including-tussenvoegsel (at) microsoft.com</p>

              <p style="font-family:helvetica; display:flex; justify-content:flex-end">Factoid: my ORCID (326797241) is a prime number; it is expressible as the sum of two squares (1715 and 17996); and it is the square root (hypothenuse) of the sum of two squares (61726280 and 320914791). Yay.</p>
            </div>
          </div>

        </div> <!-- main-text -->

    </div>

  </div> <!-- content -->


  </body>
</html>

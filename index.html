<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="keywords" content="Adrian, de Wynter, Adrian de Wynter, Bort">
  <meta name="description" content="Adrian de Wynter's home page.">
  <meta name="google-site-verification" content="UhAhnWA093u8hR6zIezUozC_aTIl1dT1rPX8UEuOdDg" />
  <title style="font-family:helvetica">Adrian de Wynter</title>
  <meta name="viewport" content="width=device-width">
  <link rel="icon" type="image/png" href="dewynter.jpg"/>
  <style>
    p {
      margin-top: 2.5%;
      margin-right: 2.5%;
      margin-left: 2.5%;
      margin-bottom: 2.5%;
    }
    h2 {
      font-family:helvetica;
      margin-top: 2.5%;
    }
  </style>
</head>

<body>

  <div class="page-content">
    <div class="wrap">
    <h1 >Adrian de Wynter<a href="dewynter.jpg" target="_blank"><img style="float:right; width:200px; margin-left: 5%" src="dewynter.jpg"/></a></h1>
    <h>/ˈeɪdrɪən təʋɪnˈtər/</p> <!-- No, it's not a "d", and it's not an "a" sound, either. It's a dialect. But I mean everyone pronounces it however they want, so I kinda got used to it. :) -->

    <p>I am a senior Data & Applied Scientist at Microsoft. At the moment I am working on projects addressing natural language understanding/generation and neural architecture search.</p>

    <p>My primary research interest lies within computation, and specifically, the development of algorithms and meta-algorithms for machine learning. My approach is mainly intuitionistic in nature, contrasting with some other formalisms used in this field. Namely, algorithms should have provable guarantees of complexity and convergence via construction, and this proof must be closely-related to a <em>computable</em> (e.g., realistic, decidable, production) scenario. This has the advantage of providing feasible, meaningful statements about complex problems, while at the same time circumventing mathematical results that are rarely, or at all, seen in practice.</p>

    <p>I'm a strong proponent of training small and efficient models, as opposed to overspecified networks--which I call <a href="https://www.youtube.com/watch?v=_oNgyUAEv0Q">Jurassic</a> networks--via the development of algorithms with provable optimality guarantees. Here "efficient" would mean "only as big as needed for the task". This is important because the power required to train these huge models can be translated directly into tons of carbon emitted into the atmosphere, and it's <a href="https://arxiv.org/abs/1906.02243">devastating</a> to the environment.</p>

    <p> Although I <a href="https://arxiv.org/pdf/1908.09942.pdf">showed</a> that finding a globally optimal solution to this problem is undecidable in its general form, I have also <a href="https://arxiv.org/pdf/2010.08512">proved</a> that for several interesting cases it is possible to find approximation algorithms that give near-optimal solutions in polynomial time--going as far as <a href="https://arxiv.org/pdf/2010.10499">applying</a> these results to the well-known BERT language model and reaching a new state-of-the-art on model compression. This last contribution was later adapted for quantum circuit optimization in a rather fantastic <a href="https://github.com/XanaduAI/QHack2021/issues/73">work</a> by folks at ORNL.</p>

    <p>Other research interests of mine are related to preserving endangered languages, as well as the computational aspects of memes-- concretely, their dynamics.</p>

<p></p>
<hr>

<h2>Posts</h2>
<p> I've found it useful to have a series of "posts" on the work I do, to make it more accessible and share my passion for mathematics. Especially since I don't have any social media (does LinkedIn count?)...</p>
<ul>
  <li><a href="https://adewynter.github.io/notes/decidability_of_nas.html">Some Computational Aspects of NAS</a>: A post on how hard neural architecture search (NAS) and machine learning can be, from a computational perspective. It also discusses the workarounds and applications of this result, with a particular emphasis on why some NAS approaches do not do better than random search. This is a summary of my poorly-titled, ever-misinterpreted paper "On The Bounds of Function Approximations."
  </li>
  <li><a href="https://adewynter.github.io/notes/bort_algorithms_and_applications.html">Bort: Algorithms and Applications</a>: a post on the algorithms used to obtain Bort, an optimally compressed version of the BERT language model. This can be viewed as a summary of my papers "Optimal Subarchitecture Extraction for BERT", "An Algorithm for Learning Smaller Representations of Models With Scarce Data", and "An Approximation Algorithm for Optimal Subarchitecture Extraction", albeit less concise than these titles, if you can believe it.</li>
</ul>

<p></p>
<hr>

<h2>Publications and Talks</h2>
<p>Following Larry Wasserman's <a href="http://stat.cmu.edu/~larry/Peer-Review.pdf">essay</a>, I invite comments on the papers below. Feel free to email me.</p>

  <table>

    <tr>
      <td><b>2021</b></td>
      <td>
        <b>Turing Completeness and <i>Sid Meier's Civilization</i></b><br />Adrian de Wynter<br />
        Preprint<br />
        <a href="https://arxiv.org/abs/2104.14647">[pdf]</a>
        <a href="bibtex/civ.html">[BibTex]</a>
        <a href="https://adewynter.github.io/notes/img/BB3.gif">[The Turing Machine in Action]</a>
      </td>
    </tr>

    <tr>
      <td><b>2021</b></td>
      <td>
        <b>Bort: Algorithms and Applications</b>
        <br/>Adrian de Wynter
        <br/>Invited talk at the 2021 Alexa Prize Summit<br />
      </td>
    </tr>


    <tr>
      <td><b>2020</b></td>
      <td>
        <b>Optimal Subarchitecture Extraction for BERT</b><br />Adrian de Wynter and Daniel J. Perry<br />
        Preprint<br />
        <a href="https://arxiv.org/pdf/2010.10499">[pdf]</a>
        <a href="bibtex/osebert.html">[BibTex]</a>
        <a href="https://github.com/alexa/bort">[Code]</a>
      </td>
    </tr>

    <tr>
      <td><b>2020</b></td>
      <td>
        <b>An Algorithm for Learning Smaller Representations of Models With Scarce Data</b><br />Adrian de Wynter<br />
        Preprint<br />
        <a href="https://arxiv.org/pdf/2010.07990">[pdf]</a>
        <a href="bibtex/agora.html">[BibTex]</a>
      </td>
    </tr>

    <tr>
      <td><b>2020</b></td>
      <td>
        <b>An Approximation Algorithm for Optimal Subarchitecture Extraction</b><br />Adrian de Wynter<br />
        Preprint<br />
        <a href="https://arxiv.org/pdf/2010.08512">[pdf]</a>
        <a href="bibtex/ose.html">[BibTex]</a>
      </td>
    </tr>

    <tr>
      <td><b>2020</b></td>
      <td>
        <b>Mischief: A Simple Black-Box Attack Against Transformer Architectures</b><br />Adrian de Wynter<br />
        Preprint<br />
        <a href="https://arxiv.org/pdf/2010.08542">[pdf]</a>
        <a href="bibtex/mischief.html">[BibTex]</a>
      </td>
    </tr>

    <tr>
      <td><b>2020</b></td>
      <td>
        <b>Harder Performance Measures for Language Models</b>
        <br/>Adrian de Wynter
        <br/>Invited talk at the 2020 Alexa Prize Summit (UPDATE: canceled due to COVID-19, stay safe!)<br />
      </td>
    </tr>

    <tr>
      <td><b>2019</b></td>
      <td>
        <b>On the Bounds of Function Approximations</b><br />Adrian de Wynter<br />ICANN 2019 (oral presentation)<br />
        <a href="https://arxiv.org/pdf/1908.09942.pdf">[pdf]</a>
        <a href="bibtex/fa.html">[BibTex]</a>
      </td>
    </tr>

    <tr>
      <td><b>2019</b></td>
      <td>
        <b>Leveraging External Knowledge for Out-Of-Vocabulary Entity Labeling</b><br />Adrian de Wynter and Lambert Mathias<br />Preprint<br />
        <a href="https://arxiv.org/pdf/1908.09936.pdf">[pdf]</a>
        <a href="bibtex/oov.html">[BibTex]</a>
      </td>
    </tr>

</table>

<p></p>
<hr>

<h2>Service</h2>
<table>
  <tr>
    <td><b>2020</b></td>
    <td>
      <b>International Conference on Artificial Neural Networks </b>
      <br/>Adrian de Wynter
      <br/>Reviewer<br />
    </td>
  </tr>

  <tr>
    <td><b>2020</b></td>
    <td>
      <b>Annual Conference of the Association for Computational Linguistics </b>
      <br/>Adrian de Wynter
      <br/>Reviewer<br />
    </td>
  </tr>

</table>

<p></p>
<hr>

<h2>Selected Press Coverage</h2>

<p>Some coverage of the work I do, in case my posts remain as confusing as the original papers.</p>
<ul>
  <li>
    <b><a href="https://www.amazon.science/blog/a-version-of-the-bert-language-model-thats-20-times-as-fast">A version of the BERT language model that’s 20 times as fast</a></b> - Another post edited by Larry Hardesty. This one talks about Bort.
  </li>

  <li>
    <b><a href="https://www.infoq.com/articles/state-art-automl/">State of the Art in Automated Machine Learning</a></b> - This is an interview I, along with other researchers, gave for InfoQ around AutoML. It's so interesting to see people of such different backgrounds arriving to the same conclusions :)
  </li>

  <li>
    <b><a href="https://www.infoq.com/news/2019/11/alexa-genetic-deep-learning/">Alexa Research Paper Shows Genetic Algorithms Offer Best Solution for Neural Network Optimization</a></b> - This post sums up very nicely my work around NAS/ASP/FA.
  </li>

  <li>
    <b><a href="https://venturebeat.com/2019/09/23/amazon-researchers-say-evolutionary-approach-improves-the-selection-of-ai-models/">Amazon researchers say evolutionary approach improves the selection of AI models</a></b> - From Venturebeat.
  </li>
  <li>
      <b><a href="https://www.amazon.science/blog/how-to-construct-the-optimal-neural-architecture-for-your-machine-learning-task">How to Construct the Optimal Neural Architecture for Your Machine Learning Task</a></b> - A post edited by the awesome Larry Hardesty.
  </li>
</ul>

<p></p>
<hr>

<p>Contact: first-initial-full-last-name (at) microsoft.com<br/>
  Factoid: my ORCID (326797241) is a prime number; it is expressible as the sum of two squares (1715 and 17996); and it is the square root (hypothenuse) of the sum of two squares (61726280 and 320914791). Yay.<br/>
  Last updated: Dec '21.<br/></p>

<font size=0 color=#ffffff>Adrian, de Wynter, Adrian de Wynter, NAS, meta-learning, Bort, Agora, intuitionistic machine learning</font>

</body>

</html>
<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>Adrian de Wynter</title>
  <meta name="viewport" content="width=device-width">
  <link rel="icon" type="image/png" href="dewynter.jpg"/>
</head>

<meta name="keywords" content="Adrian, de Wynter, Adrian de Wynter, Bort">
<meta name="description" content="Adrian de Wynter's home page.">

<body>

  <!-- <nav class="navbar">
    <ul>
      <li class="active"><a href="/">Home</a></li>
      <li><a href="/code">Code</a></li>
    </ul>
  </nav> -->

  <div class="page-content">
    <div class="wrap">
    <h1 >Adrian de Wynter<a href="dewynter.jpg" target="_blank"><img style="float: right; width:200px;" src="dewynter.jpg"/></a></h1>
    <h>/ˈeɪdrɪən təʋɪnˈtər/</p>

    <p>I am an Applied Scientist in Amazon Alexa's Secure AI Foundations (SAIF) organization.</p>
    <p>My primary research interest lies within computation, and (mostly) machine learning. At the moment I am working on projects addressing natural language understanding and neural architecture search. 
    On the former, I'm focusing mainly on deep learning, and the intersection between privacy, understanding, and bias. On the latter, I do research on the computational limits of meta-learning.</p>
    <p>I'm a strong proponent of training small and efficient models, as opposed to overspecified networks--which I call <a href="https://www.youtube.com/watch?v=_oNgyUAEv0Q">Jurassic</a> networks--via the development of algorithms with provable optimality guarantees. Here "efficient" would mean "only as big as needed for the task". This is important because the power required to train these huge models can be translated directly into tons of carbon emitted into the atmosphere, and it's <a href="https://arxiv.org/abs/1906.02243">devastating</a>.</p>

    <p> <a href="notes/bort_algorithms_and_applications.html">Here's</a> a blog post on the algorithms behid Bort.</p>

    <p>Following Larry Wasserman's <a href="http://stat.cmu.edu/~larry/Peer-Review.pdf">essay</a>, I invite comments on the papers below. Feel free to email me.</p>

  <h2>Publications and Talks</h2>
<table>


    <tr>
      <td><b>2020</b></td>
      <td>
        <b>Optimal Subarchitecture Extraction for BERT</b><br />Adrian de Wynter and Daniel J. Perry<br />
        Preprint<br />
        <a href="https://arxiv.org/pdf/2010.10499">[pdf]</a>
        <a href="bibtex/osebert.html">[BibTex]</a>
        <a href="https://github.com/alexa/bort">[Code]</a>
      </td>
    </tr>

    <tr>
      <td><b>2020</b></td>
      <td>
        <b>An Algorithm for Learning Smaller Representations of Models With Scarce Data</b><br />Adrian de Wynter<br />
        Preprint<br />
        <a href="https://arxiv.org/pdf/2010.07990">[pdf]</a>
        <a href="bibtex/agora.html">[BibTex]</a>
      </td>
    </tr>

    <tr>
      <td><b>2020</b></td>
      <td>
        <b>An Approximation Algorithm for Optimal Subarchitecture Extraction</b><br />Adrian de Wynter<br />
        Preprint<br />
        <a href="https://arxiv.org/pdf/2010.08512">[pdf]</a>
        <a href="bibtex/ose.html">[BibTex]</a>
      </td>
    </tr>

    <tr>
      <td><b>2020</b></td>
      <td>
        <b>Mischief: A Simple Black-Box Attack Against Transformer Architectures</b><br />Adrian de Wynter<br />
        Preprint<br />
        <a href="https://arxiv.org/pdf/2010.08542">[pdf]</a>
        <a href="bibtex/mischief.html">[BibTex]</a>
      </td>
    </tr>

    <tr>
      <td><b>2020</b></td>
      <td>
        <b>Harder Performance Measures for Language Models</b>
        <br/>Adrian de Wynter
        <br/>Invited talk at the 2020 Alexa Prize Summit (UPDATE: canceled due to COVID-19, stay safe!)<br />
      </td>
    </tr>

    <tr>
      <td><b>2019</b></td>
      <td>
        <b>On the Bounds of Function Approximations</b><br />Adrian de Wynter<br />ICANN 2019 (oral presentation)<br />
        <a href="https://arxiv.org/pdf/1908.09942.pdf">[pdf]</a>
        <a href="bibtex/fa.html">[BibTex]</a>
      </td>
    </tr>

    <tr>
      <td><b>2019</b></td>
      <td>
        <b>Leveraging External Knowledge for Out-Of-Vocabulary Entity Labeling</b><br />Adrian de Wynter and Lambert Mathias<br />Preprint<br />
        <a href="https://arxiv.org/pdf/1908.09936.pdf">[pdf]</a>
        <a href="bibtex/oov.html">[BibTex]</a>
      </td>
    </tr>

</table>


  <h2>Service</h2>
<table>

    <tr>
      <td><b>2020</b></td>
      <td>
        <b>International Conference on Artificial Neural Networks </b>
        <br/>Adrian de Wynter
        <br/>Reviewer<br />
      </td>
    </tr>

    <tr>
      <td><b>2020</b></td>
      <td>
        <b>Annual Conference of the Association for Computational Linguistics </b>
        <br/>Adrian de Wynter
        <br/>Reviewer<br />
      </td>
    </tr>

</table>



  <p>contact: last-name-minus-first-e (at) amazon.com<br />
  </p>


<font size=0 color=#ffffff>Adrian, de Wynter, Adrian de Wynter</font>

</body>


</html>

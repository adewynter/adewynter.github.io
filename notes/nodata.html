<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>The No-Data Algorithm</title>
  <meta name="keywords" content="LLMs, No-Data Algorithm, Adrian de Wynter">
  <meta name="description" content="The No-Data Algorithm">
  <meta name="author" content="Adrian de Wynter">
  <link rel="icon" type="image/png" href="../img/dewynter.jpg"/>
  <meta content="width=device-width, initial-scale=1.0, user-scalable=no" name="viewport">
  <!-- stuff for equations -->
  <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
  <!-- readability -->
  <link href="../styles/main_index.css" media="screen" rel="stylesheet" type="text/css"/>
</head>

<body>
<div class="content">
<div class="header-wrapper">
  <header>
    <div style="display: flex; justify-content: center; text-align: center; align-items: center;">
      <h1>The No-Data Algorithm</h1>
    </div>
  </header>
</div>

<div><br><br><br></div>
<div class="main-text">
<h2>Enabling trust in LLMs-as-judges WITHOUT labelled data</h2>

<div class="text_div" style="font-family:helvetica">
<p>
<em>
This is the TL;DR of my paper <a href="https://arxiv.org/abs/2506.03083">Labelling Data with Unknown References</a>. If you are looking for the code, it is <a href="https://https://github.com/adewynter/no_data_algorithm">here</a>.
</em>
<br><br>
We have a huge problem: evaluating highly-capable LLMs is expensive. Using an LLM as labeller (i.e., LLM-as-judge) is becoming quite common, but the field is VERY divided on their effectiveness and feasibility. Add to that lack of data and pressure to scale, and you're in a pickle. 
<br><br>
There are three ways you can prove that a labeller works: statistics (which won't work without data), faith (which is not science), or a formal proof. I introduce here an algorithm that (via a formal proof, obviously) allows you to <b>decide if you can trust your LLM-as-judge</b> in a cryptographically-secure manner.
</p>
</div>

<div class="text_div">

<h1 id="outline-",style="font-family:helvetica">Outline</h1>
<ul>
<li style="font-family:helvetica"><a href="#gist">The TL;DR</a></li>
<li style="font-family:helvetica"><a href="#run">The No-Data Algorithm</a></li>
<li style="font-family:helvetica"><a href="#play">Sketches of the proofs</a></li>
<li style="font-family:helvetica"><a href="#findings">Some experimental results!</a></li>
<li style="font-family:helvetica"><a href="#implications">Implications and conclusion</a></li>
</ul>
<hr>

<div class="papers">
<div class="papers-block">
<div class="text_div">
<div class="section-label--left" id="gist">The TL;DR</div>
<p></p>

<!--<h1 id="gist",style="font-family:helvetica">The TL;DR</h1>-->
<div class="text_div" style="font-family:helvetica">
<p style="font-family:helvetica">
    <ul>
        <li style="font-family:helvetica">It is possible to label data without having <em>any</em> labels, provided that the 'thing' saying 'hey I know the labels!' (an LLM) proves it can be trusted.</li>
        <li style="font-family:helvetica">This means that an <b>LLM-as-judge can be trusted</b> to be a labeller <b>iff it passes the checks from the No-Data algorithm.</b> This is <b>cryptographically</b> secure.</li>
        <li style="font-family:helvetica">In turn this suggests that <b>we do not need labelled data to establish trust</b>. As per <a href="https://www.techinasia.com/news/microsofts-no-data-algorithm-enables-trust-without-labels">this</a> site (obviously AI-generated but a great summary), this has applications where this data is scarce (healthcare, market research, low-resource languages, etc), but models (LLMs) trained with lots of internet data COULD perform well.</li>
      </ul>
</p>

</div>
</div>
</div>
</div>



<div class="papers">
<div class="papers-block">
<div class="text_div">
<div class="section-label--left" id="run">The No-Data Algorithm</div>

<p></p>


<!--<h1 id="run",style="font-family:helvetica">Will GPT-4 Run DOOM?</h1>-->

<div class="text_div",style="font-family:helvetica">
<p style="font-family:helvetica">
<b>Picture this:</b> You do not know how to solve a problem, but someone says they can, and they tell you 'just trust me, bro'.

<b>As a</b> (self-respecting) <b>scientist</b>, that is <em>not</em> an argument that we should accept. Especially from a machine without provable guarantees of convergence (or at least, ill-characterised). <b>So how do you decide if you can trust it?</b>
<br><br>
Usually, this is done via one of three arguments. The first, is a statistical argument (i.e., a test set). But you do not have that, and hence you cannot possibly know the true answer--perhaps nobody can. The second would be via faith (the 'just trust me, bro' argument), which as we established, is not science. The third would be <b>a formal proof</b>.</p>

<p>Enter the <b>No-Data Algorithm</b>. Granted, it is technically the 'no-labels algorithm', but since you do not have labelled data, it can go either way. <em>The point is</em>, I claim that this algorithm can <b>enable you to establish trust in an evaluator</b> (say, an LLM) <b>even when you do not have labelled data</b>.
<br><br>
Does it work?
<br><br>
Yes!
<br><br>
How?
<br><br>
Just trust me, bro. 
<br><br><br><br>
Ok, not really. This post will introduce the algorithm <em>and</em> sketches of the proof. There's some experimental results, too, because peer-reviewers usually won't be convinced by proofs alone.
</p>


<p>
The <b>No-Data Algorithm</b> works as follows: you have (unlabelled) data, and a rubric of <em>what</em> the labels should be. For example, it could be something like 'does the summary cover at least five keypoints of the original text?' or 'does this string contain an even number of zeros?'. 
<br>
An <b>Evaluator</b> claims it knows the labels. A <b>Verifier</b> must check the labels of the evaluator, but <em>without knowing the labels</em>. What this algorithm does is that, via a protocol (kind of like a game) called the <b>EV Protocol</b>, gets the Evaluator and Verifier to talk to each other until the Verifier says whether it is sufficiently convinced (or not) about the Evaluator's performance.
<br>
You then run the EV Protocol on your entire dataset and get a labelled dataset <b>and</b> a statement on whether these labels can be trusted, and to what extent.
Here's a picture of the algorithm.
</p>


<div class="img_div">
<div class="polaroid">
<img src="img/nodata/no_data_pic.png" style="max-width: 90%" alt="The No-Data Algorithm" longdesc="The No-Data Algorithm">
</div>
</div>

<br>
<p>
As you probably guessed, the important bit is the EV Protocol. This is motivated by classic results in computational complexity theory, such as the Arthur-Merlin protocol from Babai, and it is a type of <b>zero-knowledge proof</b>. What that is (formally) is beyond the scope of this post, but the gist of it is that a zero-knowledge proof establishes trust via challenges. They are used in blockchain, authentication systems, and other critical areas where you should not ever assume that the interlocutor (in this case, the Evaluator) is telling the truth.
<br><br>
The EV Protocol also has challenges--two, to be exact--that are designed to provide <b>sufficient and necessary</b> information as to whether you can trust your labeller. I'll explain why they are sufficient and necessary in the next section. Here I'll introduce them, but first let me explain <em>the rest</em> of the protocol. 
<br>
The protocol works as follows:
<ol>
  <li>Given a datapoint \(x\) and the rubric \(C\), the Evaluator comes up with a new datapoint \(\tilde{x}\) that, according to it, will lead to the same label.</li>
  <li>The Verifier selects, uniformly at random, one of two challenges:
    <ol>
      <li>Does \(\tilde{x}\) have the evaluation under \(C\) as \(x\)?, or</li>
      <li>Does \(\tilde{x}\) have the same <em>structure</em> as \(x\)?
    </ol>
  <li>IF the Verifier passes the challenge, you go back to (1) and start all over. Repeat \(r\) times.</li>
</ol>
Now, assume that the challenges make sense (and are consistent and complete and so on): it is clear then that failing a challenge is an immiediate disqualification. <b>BUT</b> passing a challenge <b>does not mean trust</b>. After all, the Verifier could have gotten lucky. And, since the challenges are public, the Verifier could have fudged its answer to pass the challenges.
<br>
<br>
We can show that if the Verifier can pass one of the challenges, then the probability that the Evaluator will succeed at catching a lying Verifier will be \(1 - 1/4^r\), or around 98% for \(r=3\). 
<br>
In fact, we will <em>also show</em> if the challenges are consistent and complete (i.e., sufficient and necessary), being able to pass both challenges for any choice of \(\tilde{x}\) means that the Verifier knows the labels. So, conversely, not passing the challenges consistently = lying. 
<br>
</p>


<div class="img_div">
<div class="polaroid">
<img src="img/nodata/evpimg.png" style="max-width: 90%" alt="The EV Protocol" longdesc="Setup for the EV protocol.">
</div>
</div>

<p>
Before we get to the proofs, let me just repeat what we do:
<ol>
  <li>We challenge the verifier on every point that must be labelled. If it passes the challenge, we accept it. If it fails it, we reject it.</li>
  <li>The challenges are designed to catch liars, so it is possible for us (with a bouunded probability) to establish confidence on our evaluator <em>without the need of labels</em>.</li>
</ol>
</p>


</div>
</div></div></div>

<div class="papers">
<div class="papers-block">
<div class="text_div">
<div class="section-label--left" id="play">Sketches of the proofs</div>

<p></p>
<p></p>
<div class="text_div">
<p>We'll do a very short sketch of the proofs--don't worry, this is more to understand <em>why</em> it works. If you are interested in the full proofs, check out the paper.</p>


<p>
<b>The plan</b>: to prove correctness of the No-Data algorithm we need two things:
<ol>
  <li>Correctness of the EV protocol, and</li>
  <li>(as a corollary) correctness of the full algorithm.</li> 
</ol>

Additionally, for the proof for the EV protocol, there are two things we need to show: that it works (consistent), and that it is sufficient and necessary (complete) to establish trust.
</p>

<p>We'll start with the EV protocol, and namely that it is <b>sufficient and necessary</b>. For this, note that the rubric \(C\) is a checklist for your labels, but not the final 'decider' of these labels. After all, the way you aggregate the answers to the checklist is what decides the label, not the other way around (for example, majority vote, a non-linear function, best-of-3, etc). Let's call that \(\sigma\). It is easy to argue that your final label \(y\) will be done by first passing \(x\) into \(C\), and then this output into the aggregator: \(y = \sigma C(x)\). Let's assume that \(\sigma\) is a deterministic function.
<br>
<br>
So, nobodody knows \(\sigma\), but everyone knows \(C\). Then the checks will (should) focus on ensuring that \(C\) leads to the same valuation \(C(x)\) for any newly-generated \(\tilde{x}\). However, there are many ways one can get the same output valuation \(C(x)\) (for example, by having multiple points checking the same boxes), but that <em>does not mean that \(x\) and \(\tilde{x}\) are equivalent!!!</em>. 
Why not? Because if \(C\) has many linear terms, like 'if this, label should be 1, otherwise 0', all valuations will be the same. But with non-linear terms, like 'if this or that, then 0, otherwise 1', the \(\tilde{x}\) point that the Evaluator generates could get lucky and fulfil the 'this' or 'that' in the non-linear term--no guarantee that we are looking at the same point as \(x\). 
<br><br>
<b>So</b> we need to check for same valuation (\(C(x) =? C(\tilde{x})\)), as covered in Challenge 1 from the previous section, but also whether the points are structurally the same (to account for luck) (which is Challenge 2, above). <b>Crucially</b>, knowing Challenge 1 does not give information about Challenge 2, and viceversa. Which in turn means that being able to solve Challenge 1 (r. 2) does not necessarily mean solving the entire thing, <b>but</b> solving the entire thing does mean knowing Challenges 1 and 2. <b>Hence both challenges are sufficient and necessary.</b>
</p>

<p>Now, about <b>completeness</b>: since every challenge is chosen uniformly at random, Challenge 1 and Challenge 2 have both the same probability of appearing in front of the Evaluator. A lying Evaluator may only be able to pass one of the challenges (as per above: passing both means knowing the actual label), and sure, it can prepare in advance, <em>but it does not know in advance which one it will see</em>. So the probability of getting lucky when lying is \(1/4\). It is also easy to show that this is the worst case scenario. Repeat this \(r\) times, and you get a probability of failure (by the Verifier) of \(1/4^r\).</p>

<p>From the above, it follows that the EV Protocol bounds, up to a tight probability, confidence on a labeller per datapoint. The expensive bit is that it does cost more times to run (by a factor of \(r\)), but on the other hand it allows you to label data <b>iff</b> the Evaluator knows it (more on that later!).</p>


</div>
</div></div></div>


<div class="papers">
<div class="papers-block">
<div class="text_div">
<div class="section-label--left" id="findings">Some experimental results!</div>

<p></p>
<p></p>
<!--<h1 id="findings">Unexpected Findings: GPT-4 and Reasoning</h1>-->

<div class="text_div">
<p>
These are super short because I think the paper conveys the work better. The gist of it is that I created a synthetic data to illustrate the point, using both a decision tree and an LLM-as-a-judge (o3-mini). It evaluated a problem it did know (in-phenomenon, or IP) as well one it claimed it did (but actually didn't, out-of-phenomenon or OOP). I split the cases in 'Known' and 'Uknown', where 'Known' is, as per the title of this paper, not something we handle and is just there to illustrate what the 'true' accuracy would be if we knew the real label set. 
<br>
The predictions of the paper adjusted quite well, with the LLM and the decision tree lying through their teeth (and being caught) when the setup required it (the OOP case); and successfully (impressively, I might add, for o3-mini) solving the IP case.
</p>


<div class="img_div">
<div class="polaroid">
<img src="img/nodata/experiments_setup.png" style="max-width: 100%" alt="Experiment setup" longdesc="Experiment setup">
</div>
</div>



<div class="img_div">
<div class="polaroid">
<img src="img/nodata/results_pic.png" style="max-width: 100%" alt="Experiment results" longdesc="Results from the experimentation">
</div>
</div>


</div>
</div></div></div>

<div class="papers">
<div class="papers-block">
<div class="text_div">
<div class="section-label--left" id="implications">Implications and conclusion</div>

<p></p>
<!--<h1 id="implications">Implications</h1>-->

<div class="text_div">
<p>
So what have we learnt?
<ol>
<li>It is indeed possible to establish trust in an LLM-as-a-judge, even if you do not know the labels! This does <b>not</b> mean that you can just use them: what the No-Data algorithm says is 'you likely can/cannot trust this LLM in this task'. These challenges are cryptographically secure.</li>  
<li>You can get a labelled dataset with slightly better performance (slightly!!) if you flip the labels when you cannot trust the evaluator. It's not super duper useful, and frankly I would just stick to the No-Data algorithms measure of success.</li>
</ol>
It is worth noting that this is a very different way to think of machine learning. We used to require some sort of supervision to get the labels right--nowadays, with LLMs, there's no actual reason to do that. <b>It is not me saying this</b>: it is people using LLMs-as-judges. The truth is that the field is super divided on their use. There are accounts that they work, accounts that they do not work, and I argue that <em>it is precisely because of this division</em> that we should not care about getting a proper conclusive statement.
Instead, let's do it on a case-by-case basis: show (prove me) that you can trust your Evaluator on this task, and I will trust your results. Here's an algorithm to do that.
<br>
<br>
Aside, what I believe is most interesting is the case when the Evaluator does <em>not</em> know the label: you can flip it as much as you'd like, and all you'll get is background noise on aggregate. So you aren't really violating any laws of nature: the background entropy remains unchanged! (little physics joke for you)
<br>
The middle ground--when it gets some right and some wrong--is also why the No-Data algorithm only gives you a number: the successes. Everything else (including the threshold at which you are willing to trust a noisy evaluator) is up to the scientist. This is more or less how we are handling natural-language problems (for which this algorithm also works). 
<br>
<br>
Speaking of, one of the cool bits about the No-Data algorithm applied to judging LLMs-as-judges is that, yes, it does work, but it establishes trust <em>in the prompt.</em> So there is room for improvement: how can we establish trust <em>in the model</em>? It'll likely be more expensive and have to be a meta-meta evaluator (since the No-Data algorithm is technically a meta-evaluator), but it could create extremely reliable, cryptographically-secure online benchmarks of performance for LLM competitions that cannot be gamed.
</p>

</div>
</div>
</div>


<p></p>
<hr>

<p></p>
<p style="font-family:helvetica; display:flex; justify-content:flex-end">Adrian de Wynter</br>July 2025</p>

</div>
</div>

</body>

</html>

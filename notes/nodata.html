<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>The No-Data Algorithm</title>
  <meta name="keywords" content="LLMs, No-Data Algorithm, Adrian de Wynter">
  <meta name="description" content="The No-Data Algorithm">
  <meta name="author" content="Adrian de Wynter">
  <link rel="icon" type="image/png" href="../img/dewynter.jpg"/>
  <meta content="width=device-width, initial-scale=1.0, user-scalable=no" name="viewport">
  <!-- stuff for equations -->
  <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
  <!-- readability -->
  <link href="../styles/main_index.css" media="screen" rel="stylesheet" type="text/css"/>
</head>

<body>
<div class="content">
<div class="header-wrapper">
  <header>
    <div style="display: flex; justify-content: center; text-align: center; align-items: center;">
      <h1>The No-Data Algorithm</h1>
    </div>
  </header>
</div>

<div><br><br><br></div>
<div class="main-text">
<h2>Enabling trust in LLMs-as-judges WITHOUT labelled data</h2>

<div class="text_div" style="font-family:helvetica">
<p>
<em>
This is the TL;DR of my paper <a href="https://arxiv.org/abs/2506.03083">Labelling Data with Unknown References</a>. If you are looking for the code, it is <a href="https://github.com/adewynter/no_data_algorithm">here</a>.
</em>
<br><br>
We have a huge problem: evaluating highly-capable LLMs is expensive. Using an LLM as labeller (i.e., LLM-as-judge) is becoming quite common, but the field is VERY divided on their effectiveness and feasibility. Add to that lack of data and pressure to scale, and you're in a pickle. 
<br><br>
There are three ways you can prove that a labeller works: statistics (which won't work without data), faith (which is not science), or a formal proof. I introduce here an algorithm that (via a formal proof, obviously) allows you to <b>decide if you can trust your LLM-as-judge</b> in a mathematically rigorous manner.
</p>
</div>

<div class="text_div">

<h1 id="outline-",style="font-family:helvetica">Outline</h1>
<ul>
<li style="font-family:helvetica"><a href="#gist">The TL;DR</a></li>
<li style="font-family:helvetica"><a href="#run">The No-Data Algorithm</a></li>
<li style="font-family:helvetica"><a href="#play">Sketches of the proofs</a></li>
<li style="font-family:helvetica"><a href="#findings">Some experimental results!</a></li>
<li style="font-family:helvetica"><a href="#implications">Implications and conclusion</a></li>
</ul>
<hr>

<div class="papers">
<div class="papers-block">
<div class="text_div">
<div class="section-label--left" id="gist">The TL;DR</div>
<p></p>

<!--<h1 id="gist",style="font-family:helvetica">The TL;DR</h1>-->
<div class="text_div" style="font-family:helvetica">
<p style="font-family:helvetica">
    <ul>
        <li style="font-family:helvetica">It is possible to label data without having <em>any</em> labels, provided that the 'thing' saying 'hey I know the labels!' (an LLM) proves it can be trusted.</li>
        <li style="font-family:helvetica">This means that an <b>LLM-as-judge can be trusted</b> to be a labeller <b>iff it passes the checks from the No-Data algorithm.</b> <span class="highlight">It judges the judges</span>, if you will, in a <span class="highlight">mathematically rigorous</span> manner.</li>
        <li style="font-family:helvetica">In turn this suggests that <span class="highlight">we do not need labelled data to establish trust</span>. As per <a href="https://www.techinasia.com/news/microsofts-no-data-algorithm-enables-trust-without-labels">this</a> site (obviously AI-generated but a great summary; and they do say that humans check it), this has applications where this data is scarce (healthcare, market research, low-resource languages, etc), but models (LLMs) trained with lots of internet data COULD perform well.</li>
      </ul>
</p>

</div>
</div>
</div>
</div>



<div class="papers">
<div class="papers-block">
<div class="text_div">
<div class="section-label--left" id="run">The No-Data Algorithm</div>

<p></p>


<!--<h1 id="run",style="font-family:helvetica">Will GPT-4 Run DOOM?</h1>-->

<div class="text_div",style="font-family:helvetica">
<p style="font-family:helvetica">
<b>Picture this:</b> You do not know how to solve a problem, but someone says they can, and they tell you 'just trust me, bro'.

<b>As a</b> (self-respecting) <b>scientist</b>, that is <em>not</em> an argument that we should accept. Especially from a machine without provable guarantees of convergence (or at least, ill-characterised). <b>So how do you decide if you can trust it?</b>
<br><br>
Usually, this is done via one of three arguments. The first, is a statistical argument (i.e., a test set). But you do not have that, or the benchmarks are <a href="https://hitz-zentroa.github.io/lm-contamination/">too contaminated</a>, and hence you cannot possibly know the true answer--perhaps nobody can. The second would be via faith (the 'just trust me, bro' argument), which as we established, is not science. The third would be <b>a formal proof</b>.</p>

<p>Enter the <b>No-Data Algorithm</b>. Granted, it is technically the 'no-labels algorithm', but since you do not have labelled data, it can go either way. <em>The point is</em>, I claim that <span class="highlight">this algorithm can <b>enable you to establish trust in an evaluator</b> (say, an LLM) <b>even when you do not have labelled data</b></span>.
<br><br>
Does it work?
<br><br>
Yes!
<br><br>
How?
<br><br>
Just trust me, bro. 
<br><br><br><br>
Ok, not really. This post will introduce the algorithm <em>and</em> sketches of the proof. There's some experimental results, too, because peer-reviewers usually won't be convinced by proofs alone; <b>and</b> because it turns out that empirically, under realistic scenarios, there are some deviations from the theory that are quite interesting to look at.
</p>


<p>
The <b>No-Data Algorithm</b> works as follows: you have (unlabelled) data, and a rubric of <em>what</em> the labels should be. For example, it could be something like 'does the summary cover at least five keypoints of the original text?' or 'does this string contain an even number of zeros?'. 
<br>
An <b>Evaluator</b> claims it knows the labels. A <b>Verifier</b> must check the labels of the evaluator, but <em>without knowing the labels</em>. What this algorithm does is that, via a protocol (kind of like a game) called the <b>EV Protocol</b>, gets the Evaluator and Verifier to talk to each other until the Verifier says whether it is sufficiently convinced (or not) about the Evaluator's performance.
<br>
You then run the EV Protocol on your entire dataset and get a labelled dataset <b>and</b> a statement on whether these labels can be trusted, and to what extent. If the Verifier is not convinced, you then also have a chance to flip the label given by the Evaluator. 
Here's a picture of the algorithm.
</p>


<div class="img_div">
<div class="polaroid">
<img src="img/nodata/no_data_pic.png" style="max-width: 90%" alt="The No-Data Algorithm" longdesc="The No-Data Algorithm">
</div>
</div>

<br>
<p>
As you probably guessed, the important bit is the EV Protocol. This is motivated by classic results in computational complexity theory, such as the Arthur-Merlin protocol from Babai, and it is a type of <b>zero-knowledge proof</b>. This is why the No-Data Algorithm is mathematically rigorous secure, by the way. What a zero-knowledge proof is (formally) is beyond the scope of this post, but the gist of it is that it establishes trust via challenges. They are used in blockchain, authentication systems, and other critical areas where you should not ever assume that the interlocutor (in this case, the Evaluator) is telling the truth.
<br><br>
The EV Protocol has <span class="highlight">two challenges designed to provide <b>sufficient and necessary</b> information</span> as to whether you can trust your labeller. I'll explain why they are sufficient and necessary in the next section. Here I'll introduce them, but first let me explain <em>the rest</em> of the protocol. 
<br>
The protocol works as follows:
<ol>
  <li>Given a datapoint \(x\) and the rubric \(C\), the Evaluator comes up with a new datapoint \(\tilde{x}\) that, according to it, will lead to the same label.</li>
  <li>The Verifier selects, uniformly at random, one of two challenges:
    <ol>
      <li>Does \(\tilde{x}\) have the evaluation under \(C\) as \(x\)?, or</li>
      <li>Does \(\tilde{x}\) have the same <em>structure</em> as \(x\)?
    </ol>
  <li>IF the Evaluator passes the challenge, you go back to (1) and start all over. Repeat \(r\) times.</li>
</ol>
Now, assume that the challenges make sense (and are consistent and complete and so on): it is clear then that failing a challenge is an immediate disqualification. <b>BUT</b> passing a challenge <b>does not mean trust</b>. After all, the Evaluator could have gotten lucky. And, since the challenges are public, the Evaluator could have fudged its answer to pass the challenges.
<br>
<br>
We can show that if the Evaluator can lie or cheat to pass one of the challenges, then the probability that the Verifier will succeed at catching a lying Evaluator will be \(1 - 1/4^r\), or around 98% for \(r=3\). 
In fact, we will <em>also show</em> if the challenges are consistent and complete (i.e., sufficient and necessary), being able to pass both challenges for any choice of \(\tilde{x}\) means that the Verifier knows the labels. So, conversely, <span class="highlight">not passing the challenges consistently = lying</span>. 
</p>


<div class="img_div">
<div class="polaroid">
<img src="img/nodata/evpimg.png" style="max-width: 90%" alt="The EV Protocol" longdesc="Setup for the EV protocol.">
</div>
</div>

<p>
Before we get to the proofs, let me just repeat what we do:
<ol>
  <li>We (the Verifier, rather) challenge the Evaluator on every point that must be labelled. If it passes the challenge, we accept it. If it fails it, we reject it.</li>
  <li>The challenges are designed to catch liars, so it is possible for us (with a bounded probability) to establish confidence on our evaluator <em>without the need of labels</em>.</li>
</ol>
</p>


</div>
</div></div></div>

<div class="papers">
<div class="papers-block">
<div class="text_div">
<div class="section-label--left" id="play">Sketches of the proofs</div>

<p></p>
<p></p>
<div class="text_div">
<p>We'll do a very short sketch of the proofs--don't worry, this is more to understand <em>why</em> it works. If you are interested in the full proofs, check out the paper.</p>


<p>
<b>The plan</b>: to prove correctness of the No-Data algorithm we need two things:
<ol>
  <li>Correctness of the EV protocol, and</li>
  <li>(as a corollary) correctness of the full algorithm.</li> 
</ol>

Additionally, for the proof for the EV protocol, there are two things we need to show: that it works (consistent), and that it is sufficient and necessary (complete) to establish trust.
</p>

<p>We'll start with the EV protocol, and namely that it is <b>sufficient and necessary</b>. For this, note that the rubric \(C\) is a checklist for your labels, but not the final 'decider' of these labels. After all, the way you aggregate the answers to the checklist is what decides the label, not the other way around (for example, majority vote, a non-linear function, best-of-3, etc). Let's call that \(\sigma\). It is easy to argue that your final label \(y\) will be done by first passing \(x\) into \(C\), and then this output into the aggregator: \(y = \sigma C(x)\). Let's assume that \(\sigma\) is a deterministic function.
<br>
<br>
So, nobodody knows \(\sigma\), but everyone knows \(C\). Then the checks will (should) focus on ensuring that \(C\) leads to the same valuation \(C(x)\) for any newly-generated \(\tilde{x}\). However, there are many ways one can get the same output valuation \(C(x)\) (for example, by having multiple points checking the same boxes), but that <em>does not mean that \(x\) and \(\tilde{x}\) are equivalent!!!</em>. 
Why not? Because if \(C\) has many linear terms, like 'if this, label should be 1, otherwise 0', all valuations will be the same. But with non-linear terms, like 'if this or that, then 0, otherwise 1', the \(\tilde{x}\) point that the Evaluator generates could get lucky and fulfil the 'this' or 'that' in the non-linear term--no guarantee that we are looking at the same point as \(x\). 
<br><br>
<b>So</b> we need to check for same valuation (\(C(x) =? C(\tilde{x})\)), as covered in Challenge 1 from the previous section, but also whether the points are structurally the same (to account for luck) (which is Challenge 2, above). <b>Crucially</b>, knowing Challenge 1 does not give information about Challenge 2, and viceversa. Which in turn means that being able to solve Challenge 1 (r. 2) does not necessarily mean solving the entire thing, <b>but</b> solving the entire thing does mean knowing Challenges 1 and 2. <b>Hence both challenges are sufficient and necessary.</b>
</p>

<p>Now, about <b>completeness</b>: since every challenge is chosen uniformly at random, Challenge 1 and Challenge 2 have both the same probability of appearing in front of the Evaluator. A lying Evaluator may only be able to pass one of the challenges (as per above: passing both means knowing the actual label), and sure, it can prepare in advance, <em>but it does not know in advance which one it will see</em>. So the probability of getting lucky when lying is \(1/4\). It is also easy to show that this is the worst case scenario. Repeat this \(r\) times, and you get a probability of failure (by the Verifier) of \(1/4^r\).</p>

<p>From the above, it follows that the EV Protocol bounds, up to a tight probability, confidence on a labeller per datapoint. The expensive bit is that it does cost more times to run (by a factor of \(r\)), but on the other hand it allows you to label data <b>iff</b> the Evaluator knows it (more on that later!).</p>


</div>
</div></div></div>


<div class="papers">
<div class="papers-block">
<div class="text_div">
<div class="section-label--left" id="findings">Some experimental results!</div>

<p></p>
<p></p>
<!--<h1 id="findings">Unexpected Findings: GPT-4 and Reasoning</h1>-->

<div class="text_div">
<p>
These are super short because I think the paper conveys the work better. The gist of it is that I ran two experiments: one to show that the empirical results adjusted to the theory; and another with an application (low-resource data classification!).
</p>

<p><b>For the empirical study</b>, I created a synthetic data to illustrate the point, using both a decision tree and an LLM-as-a-judge (o3-mini) as evaluators, and a regex as verifier. It evaluated a problem it did know (in-phenomenon, or IP) as well one it claimed it did (but actually didn't, out-of-phenomenon or OOP). I split the cases in 'knowable' and 'unknowable', where 'knowable' is, as per the title of this paper, not something we handle and is just there to illustrate what the 'true' accuracy would be if we knew the real label set. 
<br>
The predictions of the paper adjusted quite well, with the LLM and the decision tree lying through their teeth (and being caught) when the setup required it (the OOP case); and successfully (impressively, I might add, for o3-mini) solving the IP case. The paper has further studies on DeepSeek, Qwen 2.5B, and GPT-4o. Check it out if you're interested.
</p>


<div class="img_div">
<div class="polaroid">
<img src="img/nodata/experiments_setup.png" style="max-width: 90%" alt="Experiment setup" longdesc="Experiment setup">
</div>
</div>



<div class="img_div">
<div class="polaroid">
<img src="img/nodata/results_pic.png" style="max-width: 90%" alt="Experiment results" longdesc="Results from the experimentation">
</div>
</div>


<p><b>For the low-resource study</b>, I chose West Frisian. It is close to my heart and I think that it deserves more attention. Even when it is spoken by about half a million people, those who know how to write it are even fewer. Hence, as per the taxonomy of <a href="https://aclanthology.org/2020.acl-main.560/">Joshi et al.</a>, it is an extremely low-resource language, where--as called out in the paper for many languages--<b>synthetic data will do more harm than good</b>. Hence, it is a great case study! <span class="highlight">Can you enable others to work on scarce data scenarios</span>? Here, 'enable' means 'they can trust the tool they are using'.
<br>
<br>
So for this one I got native speakers to translate a multi-domain dataset (MMLU, OpenOrca, WildChat, and OpenCode) of prompts and outputs. Both the Evaluator and the Verifier are LLMs (GPT-4.1). 
<b>The task for the LLM-as-judge was to evaluate whether the output was 'good'</b> with respect to a rubric (below). 
<b>The task for the No-Data Algorithm was to <span class="highlight"><em>judge the judge</em></span></b> to determine trustworthiness. So I created a separate rubric (also below) to determine how and when a model would lie. Lo and behold, the results stuck to the theory, with the LLM-as-judge showing near-random performance when the data was OOP, and somewhat okish performance in IP. <span class="highlight">Crucially, in this experimeent the values of the rubric were unknown</span>; and the LLMs had to label it themselves!
<br>
<br>
The setup, however, was <b>slightly different than the theoretical work</b>: for example, in the empirical test the criteria was evaluated by an oracle, while here <b>both LLMs need to score the criteria <em>first</em>, and then do the whole EV protocol</b>. There was also the matter of LLMs doing whatever they want (because <em>they are machines, they really really don't know what you are talking about</em>). So, for example, if you put exemplars in the prompt, the LLMs will never pass any challenge. Turns out that they were lying for <em>both</em> IP and OOP, and the No-Data algorithm caught that!!<small>it took me two weeks to figure out why the algorithm kept saying this</small>
<br>
Of course, after removing the exemplars, things made more sense. But that took me a while to debug. Turns out that the outputs for every single criterion were almost the same, because it was mimicking the exemplars and not listening to the prompt. See what I mean about them not knowing what you are talking about?
<br>
<br>
It is interesting--while it could be argued that <b>verifying an evaluator that is also a verifier leads to a circular argument</b>, do note that these are <b>two different problems</b>. And, in fact, <b>empirically the results reflected so</b>. <small>Plus if you are icky about it just use another model lol it's not like there is only one LLM out there. </small>
Anyway, the takeaway is that <span class="highlight">we can use the No-Data Algorithm to label data in low-resource languages</span>! And, more importantly, in <span class="highlight"><b>realistic</b> scenarios</span> . 
</p>


<div class="img_div">
<div class="polaroid">
<img src="img/nodata/nl_rubric_pic.png" style="max-width: 80%" alt="Experiment results" longdesc="Results from the experimentation">
</div>
</div>


<div class="img_div">
<div class="polaroid">
<img src="img/nodata/nl_results_pic.png" style="max-width: 80%" alt="Experiment setup" longdesc="Experiment setup">
</div>
</div>


<br>
<p>
There is something cool, too. So far we have set up the problem so that one of the evaluators lies. <b>What if it's not lying, just wrong?</b> Well, the No-Data algorithm can also catch that: low success rates (like, <em>really</em> low) mean deception, but sort-of-low indicate lack of knowledge. For this I compared GPT-4.1 (which kinda knows West Frisian) with Qwen 2.5B (which definitely does not know West Frisian). See the table below for the results.
</p>


<div class="img_div">
<div class="polaroid">
<img src="img/nodata/nl_results_pic_all.png" style="max-width: 80%" alt="Experiment setup" longdesc="Experiment setup">
</div>
</div>


</div>
</div></div></div>

<div class="papers">
<div class="papers-block">
<div class="text_div">
<div class="section-label--left" id="implications">Implications and conclusion</div>

<p></p>
<!--<h1 id="implications">Implications</h1>-->

<div class="text_div">
<p>
So what have we learnt?
<ol>
<li><span class="highlight">It is indeed possible to establish trust in an LLM-as-a-judge, even if you do not know the labels</span>! This does <b>not</b> mean that you can just use them: what the No-Data algorithm says is 'you likely can/cannot trust this LLM in this task'. <b>These challenges are mathematically rigorous</b>. As in, they are <em>proven</em> to be what you need.</li>  
<li>You can get a labelled dataset with slightly better performance (slightly!!) if you flip the labels when you cannot trust the evaluator. It's not super duper useful, and frankly I would just stick to the No-Data algorithms measure of success.</li>
</ol>
It is worth noting that <span class="highlight">this is a very different way to think of machine learning</span>. We used to require some sort of supervision (someone telling you if it is correct) to get 'learning' right. Nowadays, with LLMs and their bloated internet knowledge, there's no actual reason to do that. <b>It is not me saying this</b>: it is people using LLMs-as-judges. And, I mean, the field is super divided on their use. There are accounts that they work, accounts that they do not work, and I argue that <em>it is precisely because of this division</em> that <b>we should not care about getting a properly conclusive statement</b>.
Instead, let's do it on a case-by-case basis: show (prove me) that you can trust your Evaluator on this task, and I will trust your results. Do science the right way, damn it. Here's an algorithm to do that.
<br>
<br>
Aside, what I believe is most interesting is the case when the Evaluator does <em>not</em> know the label: you can flip it as much as you'd like, and all you'll get is background noise on aggregate. So you aren't really violating any laws of nature: the entropy of the system remains unchanged! (little physics joke for you <small>and yes I know that measurement will affect the information content of the system it is a joke chill</small>)
<br>
<br>
The middle ground--when it gets some right and some wrong--is also why the No-Data algorithm only gives you a number: the successes. Everything else (including the threshold at which you are willing to trust a noisy evaluator) is up to the scientist. This is more or less how we are handling natural-language problems (for which this algorithm also works, as seen). What matters, really, is that <span class="highlight">the No-Data algorithm enables you to use LLMs-as-judges in areas where labelled data is extremely scarce</span>, like market research, healthcare, and low-resource languages. 
<br>
<br>
Speaking of, one of the cool bits about the No-Data algorithm applied to judging LLMs-as-judges is that, yes, it does work, but it establishes trust <em>in the prompt,</em> not the LLM. So there is room for improvement: how can we establish trust <em>in the model</em>? It'll likely be more expensive and have to be a meta-meta evaluator (since the No-Data algorithm is technically a meta-evaluator), but it could create extremely reliable, cryptographically-secure online benchmarks of performance for LLM competitions that cannot be gamed.
</p>

</div>
</div>
</div>


<p></p>
<hr>

<p></p>
<p style="font-family:helvetica; display:flex; justify-content:flex-end">Adrian de Wynter</br>July 2025</p>

</div>
</div>

</body>

</html>
